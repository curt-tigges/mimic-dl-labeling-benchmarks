{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9626b3",
   "metadata": {},
   "source": [
    "## Transformers for ICD Prediction from MIMIC III\n",
    "\n",
    "Using Transformers pre-trained model for medical code predictions using MIMIC III Clinical notes data\n",
    "\n",
    "- Data preprocessing based on CAML: https://github.com/jamesmullenbach/caml-mimic\n",
    "- Pytorch training code based on : https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63e76bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 19 05:08:04 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.14       Driver Version: 470.14       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 70%   61C    P0    36W / 200W |    821MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       536    C+G   ...bbwe\\HxCalendarAppImm.exe    N/A      |\n",
      "|    0   N/A  N/A      4308    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      5532    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      5564    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      6028    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A      7360    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7752    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8172    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      8276    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      8540    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A      9188    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9236    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A      9636    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A      9772    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     10008    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     10056    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     10204    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A     10680    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11500    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     11796    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     12716    C+G   ...er_engine\\wallpaper32.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba094fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba4245bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee5aa4",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc907d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABELS</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7908</td>\n",
       "      <td>182396</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>287.5;584.9;45.13</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11231</td>\n",
       "      <td>183363</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>96.71;401.9;272.4</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3184</td>\n",
       "      <td>144347</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>530.81</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24427</td>\n",
       "      <td>177066</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>96.71;V58.61;276.2;96.04</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1262</td>\n",
       "      <td>183373</td>\n",
       "      <td>admission date discharge date service neurolog...</td>\n",
       "      <td>V58.61;244.9;414.01;401.9;96.71;427.31</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  HADM_ID                                               TEXT  \\\n",
       "0        7908   182396  admission date discharge date date of birth se...   \n",
       "1       11231   183363  admission date discharge date date of birth se...   \n",
       "2        3184   144347  admission date discharge date date of birth se...   \n",
       "3       24427   177066  admission date discharge date date of birth se...   \n",
       "4        1262   183373  admission date discharge date service neurolog...   \n",
       "\n",
       "                                   LABELS  length  \n",
       "0                       287.5;584.9;45.13     105  \n",
       "1                       96.71;401.9;272.4     106  \n",
       "2                                  530.81     117  \n",
       "3                96.71;V58.61;276.2;96.04     148  \n",
       "4  V58.61;244.9;414.01;401.9;96.71;427.31     156  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to where you store mimic3 data\n",
    "MIMIC_3_DIR = 'D:/OneDrive/Academic/CS598-DLH/caml-mimic/mimicdata/mimic3'\n",
    "\n",
    "train_df = pd.read_csv('%s/train_50.csv' % MIMIC_3_DIR)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d493443",
   "metadata": {},
   "source": [
    " ## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e54b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABELS</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7908</td>\n",
       "      <td>182396</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>[287.5, 584.9, 45.13]</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11231</td>\n",
       "      <td>183363</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>[96.71, 401.9, 272.4]</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3184</td>\n",
       "      <td>144347</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>[530.81]</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24427</td>\n",
       "      <td>177066</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>[96.71, V58.61, 276.2, 96.04]</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1262</td>\n",
       "      <td>183373</td>\n",
       "      <td>admission date discharge date service neurolog...</td>\n",
       "      <td>[V58.61, 244.9, 414.01, 401.9, 96.71, 427.31]</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  HADM_ID                                               TEXT  \\\n",
       "0        7908   182396  admission date discharge date date of birth se...   \n",
       "1       11231   183363  admission date discharge date date of birth se...   \n",
       "2        3184   144347  admission date discharge date date of birth se...   \n",
       "3       24427   177066  admission date discharge date date of birth se...   \n",
       "4        1262   183373  admission date discharge date service neurolog...   \n",
       "\n",
       "                                          LABELS  length  \n",
       "0                          [287.5, 584.9, 45.13]     105  \n",
       "1                          [96.71, 401.9, 272.4]     106  \n",
       "2                                       [530.81]     117  \n",
       "3                  [96.71, V58.61, 276.2, 96.04]     148  \n",
       "4  [V58.61, 244.9, 414.01, 401.9, 96.71, 427.31]     156  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split labels by \";\", then convert to list\n",
    "def split_lab (x):\n",
    "    #print(x)\n",
    "    return x.split(\";\")\n",
    "\n",
    "train_df['LABELS'] = train_df['LABELS'].apply(split_lab)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56bbfe56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['38.93'],\n",
       "       ['428.0'],\n",
       "       ['427.31'],\n",
       "       ['414.01'],\n",
       "       ['96.04']], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check top 50 code\n",
    "top_50 = pd.read_csv('%s/TOP_50_CODES.csv' % MIMIC_3_DIR)\n",
    "\n",
    "top_50.head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e66f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load multi label binarizer for one-hot encoding\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "#labels_onehot = mlb.fit_transform(train_df.pop('LABELS'))\n",
    "#labels_onehot[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8516e8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>length</th>\n",
       "      <th>038.9</th>\n",
       "      <th>244.9</th>\n",
       "      <th>250.00</th>\n",
       "      <th>272.0</th>\n",
       "      <th>272.4</th>\n",
       "      <th>276.1</th>\n",
       "      <th>...</th>\n",
       "      <th>96.04</th>\n",
       "      <th>96.6</th>\n",
       "      <th>96.71</th>\n",
       "      <th>96.72</th>\n",
       "      <th>99.04</th>\n",
       "      <th>99.15</th>\n",
       "      <th>995.92</th>\n",
       "      <th>V15.82</th>\n",
       "      <th>V45.81</th>\n",
       "      <th>V58.61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7908</td>\n",
       "      <td>182396</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11231</td>\n",
       "      <td>183363</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3184</td>\n",
       "      <td>144347</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24427</td>\n",
       "      <td>177066</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1262</td>\n",
       "      <td>183373</td>\n",
       "      <td>admission date discharge date service neurolog...</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  HADM_ID                                               TEXT  \\\n",
       "0        7908   182396  admission date discharge date date of birth se...   \n",
       "1       11231   183363  admission date discharge date date of birth se...   \n",
       "2        3184   144347  admission date discharge date date of birth se...   \n",
       "3       24427   177066  admission date discharge date date of birth se...   \n",
       "4        1262   183373  admission date discharge date service neurolog...   \n",
       "\n",
       "   length  038.9  244.9  250.00  272.0  272.4  276.1  ...  96.04  96.6  96.71  \\\n",
       "0     105      0      0       0      0      0      0  ...      0     0      0   \n",
       "1     106      0      0       0      0      1      0  ...      0     0      1   \n",
       "2     117      0      0       0      0      0      0  ...      0     0      0   \n",
       "3     148      0      0       0      0      0      0  ...      1     0      1   \n",
       "4     156      0      1       0      0      0      0  ...      0     0      1   \n",
       "\n",
       "   96.72  99.04  99.15  995.92  V15.82  V45.81  V58.61  \n",
       "0      0      0      0       0       0       0       0  \n",
       "1      0      0      0       0       0       0       0  \n",
       "2      0      0      0       0       0       0       0  \n",
       "3      0      0      0       0       0       0       1  \n",
       "4      0      0      0       0       0       0       1  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change label to one-hot encoding per code\n",
    "train_df = train_df.join(\n",
    "            pd.DataFrame.sparse.from_spmatrix(\n",
    "                mlb.fit_transform(train_df.pop('LABELS')),\n",
    "                index=train_df.index,\n",
    "                columns=mlb.classes_))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6e6968a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>length</th>\n",
       "      <th>038.9</th>\n",
       "      <th>244.9</th>\n",
       "      <th>250.00</th>\n",
       "      <th>272.0</th>\n",
       "      <th>272.4</th>\n",
       "      <th>276.1</th>\n",
       "      <th>...</th>\n",
       "      <th>96.6</th>\n",
       "      <th>96.71</th>\n",
       "      <th>96.72</th>\n",
       "      <th>99.04</th>\n",
       "      <th>99.15</th>\n",
       "      <th>995.92</th>\n",
       "      <th>V15.82</th>\n",
       "      <th>V45.81</th>\n",
       "      <th>V58.61</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7908</td>\n",
       "      <td>182396</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11231</td>\n",
       "      <td>183363</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3184</td>\n",
       "      <td>144347</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24427</td>\n",
       "      <td>177066</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1262</td>\n",
       "      <td>183373</td>\n",
       "      <td>admission date discharge date service neurolog...</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  HADM_ID                                               TEXT  \\\n",
       "0        7908   182396  admission date discharge date date of birth se...   \n",
       "1       11231   183363  admission date discharge date date of birth se...   \n",
       "2        3184   144347  admission date discharge date date of birth se...   \n",
       "3       24427   177066  admission date discharge date date of birth se...   \n",
       "4        1262   183373  admission date discharge date service neurolog...   \n",
       "\n",
       "   length  038.9  244.9  250.00  272.0  272.4  276.1  ...  96.6  96.71  96.72  \\\n",
       "0     105      0      0       0      0      0      0  ...     0      0      0   \n",
       "1     106      0      0       0      0      1      0  ...     0      1      0   \n",
       "2     117      0      0       0      0      0      0  ...     0      0      0   \n",
       "3     148      0      0       0      0      0      0  ...     0      1      0   \n",
       "4     156      0      1       0      0      0      0  ...     0      1      0   \n",
       "\n",
       "   99.04  99.15  995.92  V15.82  V45.81  V58.61  \\\n",
       "0      0      0       0       0       0       0   \n",
       "1      0      0       0       0       0       0   \n",
       "2      0      0       0       0       0       0   \n",
       "3      0      0       0       0       0       1   \n",
       "4      0      0       0       0       0       1   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert columns to list of one hot encoding\n",
    "icd_classes_50 = mlb.classes_\n",
    "\n",
    "train_df['labels'] = train_df[icd_classes_50].values.tolist()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03639f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if one-hot encoding is correct\n",
    "len(train_df.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0592da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>admission date discharge date service neurolog...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  admission date discharge date date of birth se...   \n",
       "1  admission date discharge date date of birth se...   \n",
       "2  admission date discharge date date of birth se...   \n",
       "3  admission date discharge date date of birth se...   \n",
       "4  admission date discharge date service neurolog...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert into 2 columns dataframe\n",
    "train_df = pd.DataFrame(train_df, columns=['TEXT', 'labels'])\n",
    "train_df.columns=['text', 'labels']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaad265",
   "metadata": {},
   "source": [
    "### Prepare Eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978a24f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                    text  \\\n",
       "0     admission date discharge date date of birth se...   \n",
       "1     admission date discharge date service neurosur...   \n",
       "2     admission date discharge date date of birth se...   \n",
       "3     admission date discharge date date of birth se...   \n",
       "4     admission date discharge date date of birth se...   \n",
       "...                                                 ...   \n",
       "1568  admission date discharge date date of birth se...   \n",
       "1569  admission date discharge date date of birth se...   \n",
       "1570  admission date discharge date date of birth se...   \n",
       "1571  admission date discharge date date of birth se...   \n",
       "1572  admission date discharge date date of birth se...   \n",
       "\n",
       "                                                 labels  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "1568  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1569  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1570  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "1571  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1572  [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[1573 rows x 2 columns]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same as train data preparation, but for evaluation\n",
    "eval_df = pd.read_csv('%s/dev_50.csv' % MIMIC_3_DIR)\n",
    "\n",
    "eval_df['LABELS'] = eval_df['LABELS'].apply(split_lab)\n",
    "\n",
    "eval_df = eval_df.join(\n",
    "            pd.DataFrame.sparse.from_spmatrix(\n",
    "                mlb.fit_transform(eval_df.pop('LABELS')),\n",
    "                index=eval_df.index,\n",
    "                columns=icd_classes_50))\n",
    "\n",
    "eval_df['labels'] = eval_df[icd_classes_50].values.tolist()\n",
    "eval_df = pd.DataFrame(eval_df, columns=['TEXT', 'labels'])\n",
    "eval_df.columns=['text', 'labels']\n",
    "\n",
    "print(len(eval_df.labels[0]))\n",
    "eval_df.describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f02c0444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                    text  \\\n",
       "0     admission date discharge date date of birth se...   \n",
       "1     admission date discharge date date of birth se...   \n",
       "2     admission date discharge date date of birth se...   \n",
       "3     admission date discharge date date of birth se...   \n",
       "4     admission date discharge date date of birth se...   \n",
       "...                                                 ...   \n",
       "1724  admission date discharge date date of birth se...   \n",
       "1725  admission date discharge date date of birth se...   \n",
       "1726  admission date discharge date date of birth se...   \n",
       "1727  admission date discharge date date of birth se...   \n",
       "1728  admission date discharge date date of birth se...   \n",
       "\n",
       "                                                 labels  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "1724  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1725  [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "1726  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1727  [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "1728  [0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, ...  \n",
       "\n",
       "[1729 rows x 2 columns]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same as train data preparation, but for evaluation\n",
    "test_df = pd.read_csv('%s/test_50.csv' % MIMIC_3_DIR)\n",
    "\n",
    "test_df['LABELS'] = test_df['LABELS'].apply(split_lab)\n",
    "\n",
    "test_df = test_df.join(\n",
    "            pd.DataFrame.sparse.from_spmatrix(\n",
    "                mlb.fit_transform(test_df.pop('LABELS')),\n",
    "                index=test_df.index,\n",
    "                columns=icd_classes_50))\n",
    "\n",
    "test_df['labels'] = test_df[icd_classes_50].values.tolist()\n",
    "test_df = pd.DataFrame(test_df, columns=['TEXT', 'labels'])\n",
    "test_df.columns=['text', 'labels']\n",
    "\n",
    "print(len(test_df.labels[0]))\n",
    "test_df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba068e",
   "metadata": {},
   "source": [
    "### Set Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24ea361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables to configure model training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "#set tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0ac40",
   "metadata": {},
   "source": [
    "### Preparing Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96169cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset for BERT class\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        \n",
    "        '''\n",
    "            set text as training data\n",
    "            set labels as targets\n",
    "        '''\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "660b88f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (8066, 2)\n",
      "TEST Dataset: (1729, 2)\n"
     ]
    }
   ],
   "source": [
    "#load df to dataset\n",
    "\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"EVAL Dataset: {}\".format(eval_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "\n",
    "training_set = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
    "evaluation_set = CustomDataset(eval_df, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95b41f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "eval_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "evaluation_loader = DataLoader(evaluation_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a2491",
   "metadata": {},
   "source": [
    "### Create model class from pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dec88e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        '''\n",
    "            Load Pretrained model here\n",
    "            Use return_dict=False for compatibility for 4.x\n",
    "        \n",
    "        '''\n",
    "        self.l1 = transformers.AutoModel.from_pretrained(\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\", return_dict=False)\n",
    "        #self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "        \n",
    "        \n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        \n",
    "        '''\n",
    "            Changed Linear Output layer to 50 based on the class\n",
    "        '''\n",
    "        self.l3 = torch.nn.Linear(768, 50)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67dd17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69e40b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c316191",
   "metadata": {},
   "source": [
    "### Train fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfd45b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a118cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                               | 0/200 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7054504156112671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                  | 1/200 [05:05<16:54:34, 305.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.2514679729938507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                  | 2/200 [10:14<16:51:52, 306.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss:  0.3509063720703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                                  | 3/200 [15:25<16:50:54, 307.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss:  0.25128602981567383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█                                                  | 4/200 [20:54<17:06:24, 314.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss:  0.267152339220047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▎                                                 | 5/200 [26:55<17:47:11, 328.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss:  0.2963433265686035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▌                                                 | 6/200 [32:47<18:04:32, 335.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss:  0.2676762044429779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▊                                                 | 7/200 [38:39<18:15:15, 340.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss:  0.30735185742378235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|██                                                 | 8/200 [44:32<18:21:21, 344.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss:  0.249963641166687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|██▎                                                | 9/200 [50:25<18:23:57, 346.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss:  0.23422323167324066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▌                                               | 10/200 [56:15<18:21:32, 347.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss:  0.17031937837600708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▋                                             | 11/200 [1:02:04<18:16:40, 348.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss:  0.19182120263576508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▉                                             | 12/200 [1:07:53<18:11:25, 348.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss:  0.25511473417282104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|███                                             | 13/200 [1:13:41<18:05:43, 348.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss:  0.1755523979663849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███▎                                            | 14/200 [1:19:27<17:57:31, 347.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss:  0.2345455288887024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▌                                            | 15/200 [1:25:12<17:49:51, 346.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss:  0.1370702087879181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▊                                            | 16/200 [1:30:59<17:43:10, 346.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss:  0.10788926482200623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|████                                            | 17/200 [1:36:45<17:36:51, 346.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Loss:  0.15599596500396729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|████▎                                           | 18/200 [1:42:31<17:30:52, 346.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Loss:  0.2236604243516922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▌                                           | 19/200 [1:48:18<17:26:08, 346.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Loss:  0.18537844717502594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▊                                           | 20/200 [1:54:07<17:21:54, 347.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Loss:  0.12852822244167328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█████                                           | 21/200 [1:59:55<17:16:45, 347.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Loss:  0.09014123678207397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█████▎                                          | 22/200 [2:05:43<17:11:35, 347.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Loss:  0.1598893702030182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████▌                                          | 23/200 [2:11:32<17:06:29, 347.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Loss:  0.14211289584636688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████▊                                          | 24/200 [2:17:20<17:01:11, 348.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Loss:  0.145722433924675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████                                          | 25/200 [2:23:09<16:55:59, 348.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Loss:  0.0938330665230751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|██████▏                                         | 26/200 [2:28:58<16:50:46, 348.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Loss:  0.11374835669994354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|██████▍                                         | 27/200 [2:34:48<16:45:46, 348.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Loss:  0.0860474482178688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|██████▋                                         | 28/200 [2:40:37<16:40:03, 348.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Loss:  0.16010911762714386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|██████▉                                         | 29/200 [2:46:25<16:34:03, 348.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Loss:  0.08742792159318924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|███████▏                                        | 30/200 [2:52:13<16:27:44, 348.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Loss:  0.06981556862592697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|███████▍                                        | 31/200 [2:57:59<16:19:39, 347.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Loss:  0.11258313804864883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|███████▋                                        | 32/200 [3:03:48<16:14:24, 348.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Loss:  0.053775232285261154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|███████▉                                        | 33/200 [3:09:37<16:09:28, 348.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Loss:  0.07253556698560715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|████████▏                                       | 34/200 [3:15:26<16:04:17, 348.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Loss:  0.07178198546171188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████▍                                       | 35/200 [3:21:15<15:58:55, 348.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Loss:  0.03385697305202484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████▋                                       | 36/200 [3:27:04<15:53:48, 348.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Loss:  0.07872681319713593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████▉                                       | 37/200 [3:32:54<15:48:20, 349.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Loss:  0.03585724160075188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█████████                                       | 38/200 [3:38:43<15:42:35, 349.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Loss:  0.07600442320108414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█████████▎                                      | 39/200 [3:44:32<15:36:44, 349.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Loss:  0.0449068583548069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█████████▌                                      | 40/200 [3:50:22<15:31:44, 349.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Loss:  0.03675665706396103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█████████▊                                      | 41/200 [3:56:13<15:27:17, 349.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Loss:  0.05216914415359497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██████████                                      | 42/200 [4:02:05<15:22:59, 350.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Loss:  0.08255185186862946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██████████▎                                     | 43/200 [4:07:57<15:18:31, 351.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Loss:  0.0767180547118187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██████████▌                                     | 44/200 [4:13:49<15:13:07, 351.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Loss:  0.04853029176592827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██████████▊                                     | 45/200 [4:19:39<15:06:00, 350.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Loss:  0.05593514442443848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|███████████                                     | 46/200 [4:25:30<15:00:22, 350.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Loss:  0.0457981713116169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████▎                                    | 47/200 [4:31:20<14:54:31, 350.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Loss:  0.028880132362246513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████▌                                    | 48/200 [4:37:11<14:48:18, 350.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Loss:  0.05149512737989426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████▊                                    | 49/200 [4:43:04<14:44:32, 351.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Loss:  0.03987174481153488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████                                    | 50/200 [4:48:54<14:37:43, 351.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Loss:  0.06379280984401703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|████████████▏                                   | 51/200 [4:54:45<14:31:19, 350.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51, Loss:  0.05793190747499466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|████████████▍                                   | 52/200 [5:00:35<14:25:09, 350.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52, Loss:  0.037550922483205795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|████████████▋                                   | 53/200 [5:06:26<14:19:27, 350.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53, Loss:  0.03309611231088638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|████████████▉                                   | 54/200 [5:12:17<14:13:32, 350.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54, Loss:  0.03876790776848793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|█████████████▏                                  | 55/200 [5:18:08<14:08:08, 350.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Loss:  0.038493163883686066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|█████████████▍                                  | 56/200 [5:24:00<14:03:02, 351.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56, Loss:  0.029800280928611755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|█████████████▋                                  | 57/200 [5:29:52<13:57:35, 351.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57, Loss:  0.020475344732403755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|█████████████▉                                  | 58/200 [5:35:43<13:51:39, 351.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58, Loss:  0.038362424820661545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████▏                                 | 59/200 [5:41:34<13:45:28, 351.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59, Loss:  0.022316182032227516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████▍                                 | 60/200 [5:47:26<13:39:56, 351.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60, Loss:  0.022490547969937325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████▋                                 | 61/200 [5:53:19<13:35:34, 352.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61, Loss:  0.03334115073084831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|██████████████▉                                 | 62/200 [5:59:13<13:30:27, 352.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62, Loss:  0.03154711797833443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███████████████                                 | 63/200 [6:05:06<13:25:32, 352.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63, Loss:  0.020208533853292465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███████████████▎                                | 64/200 [6:11:00<13:19:54, 352.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64, Loss:  0.015216421335935593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███████████████▌                                | 65/200 [6:16:53<13:14:34, 353.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65, Loss:  0.028837518766522408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████▊                                | 66/200 [6:22:48<13:09:36, 353.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66, Loss:  0.021757790818810463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|████████████████                                | 67/200 [6:28:41<13:03:42, 353.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67, Loss:  0.010986873880028725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|████████████████▎                               | 68/200 [6:34:34<12:57:15, 353.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, Loss:  0.010418160818517208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|████████████████▌                               | 69/200 [6:40:23<12:48:47, 352.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69, Loss:  0.009884159080684185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|████████████████▊                               | 70/200 [6:46:13<12:41:06, 351.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70, Loss:  0.013459840789437294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████                               | 71/200 [6:52:03<12:34:20, 350.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71, Loss:  0.01313848327845335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████▎                              | 72/200 [6:57:53<12:28:03, 350.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72, Loss:  0.008089346811175346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████▌                              | 73/200 [7:03:43<12:21:47, 350.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73, Loss:  0.011319367215037346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|█████████████████▊                              | 74/200 [7:09:33<12:15:51, 350.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74, Loss:  0.011528183706104755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|██████████████████                              | 75/200 [7:15:24<12:10:13, 350.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75, Loss:  0.01719195768237114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|██████████████████▏                             | 76/200 [7:21:14<12:04:12, 350.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76, Loss:  0.0065727028995752335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|██████████████████▍                             | 77/200 [7:27:04<11:58:07, 350.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77, Loss:  0.01821676641702652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|██████████████████▋                             | 78/200 [7:32:57<11:53:42, 351.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78, Loss:  0.0174552034586668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████▉                             | 79/200 [7:38:49<11:48:46, 351.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79, Loss:  0.005766869522631168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████▏                            | 80/200 [7:44:42<11:43:50, 351.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80, Loss:  0.01383595447987318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████▍                            | 81/200 [7:50:35<11:38:38, 352.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81, Loss:  0.008139771409332752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████▋                            | 82/200 [7:56:29<11:33:24, 352.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82, Loss:  0.009794889017939568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|███████████████████▉                            | 83/200 [8:02:22<11:28:03, 352.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83, Loss:  0.009547045454382896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████▏                           | 84/200 [8:08:16<11:22:37, 353.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84, Loss:  0.006429570727050304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████▍                           | 85/200 [8:14:09<11:17:00, 353.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85, Loss:  0.007418658584356308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████▋                           | 86/200 [8:20:00<11:09:54, 352.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86, Loss:  0.011720598675310612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████▉                           | 87/200 [8:25:52<11:03:44, 352.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87, Loss:  0.009335377253592014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|█████████████████████                           | 88/200 [8:31:46<10:58:15, 352.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88, Loss:  0.011131099425256252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|█████████████████████▎                          | 89/200 [8:37:38<10:52:22, 352.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89, Loss:  0.011614928022027016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████▌                          | 90/200 [8:43:31<10:46:31, 352.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90, Loss:  0.009798248298466206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████▊                          | 91/200 [8:49:24<10:40:53, 352.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91, Loss:  0.002647432265803218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|██████████████████████                          | 92/200 [8:55:17<10:35:13, 352.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92, Loss:  0.007744345348328352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|██████████████████████▎                         | 93/200 [9:01:10<10:29:31, 353.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93, Loss:  0.00860793236643076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|██████████████████████▌                         | 94/200 [9:07:04<10:23:57, 353.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94, Loss:  0.005952538922429085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|██████████████████████▊                         | 95/200 [9:12:57<10:18:05, 353.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95, Loss:  0.008146820589900017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████                         | 96/200 [9:18:48<10:10:54, 352.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96, Loss:  0.004257976543158293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████▎                        | 97/200 [9:24:41<10:05:30, 352.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97, Loss:  0.0036642514169216156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████████████████████████                         | 98/200 [9:30:34<9:59:39, 352.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98, Loss:  0.0022762741427868605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████▎                        | 99/200 [9:36:24<9:52:32, 352.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99, Loss:  0.007392530329525471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████                        | 100/200 [9:42:14<9:45:38, 351.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss:  0.004952679388225079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████▏                       | 101/200 [9:48:05<9:39:17, 351.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101, Loss:  0.003174605080857873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|████████████████████████▍                       | 102/200 [9:53:55<9:33:10, 350.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102, Loss:  0.008347303606569767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████████▋                       | 103/200 [9:59:46<9:27:12, 350.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103, Loss:  0.008080074563622475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████████▍                      | 104/200 [10:05:37<9:21:35, 351.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104, Loss:  0.004821994807571173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████████▋                      | 105/200 [10:11:31<9:16:58, 351.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105, Loss:  0.005777942016720772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|████████████████████████▉                      | 106/200 [10:17:25<9:12:04, 352.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 106, Loss:  0.0022347518242895603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████████████████████████▏                     | 107/200 [10:23:18<9:06:47, 352.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107, Loss:  0.0017149884952232242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████████████████████████▍                     | 108/200 [10:29:11<9:01:03, 352.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108, Loss:  0.0018553247209638357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████▌                     | 109/200 [10:35:05<8:55:21, 352.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109, Loss:  0.002112418180331588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████▊                     | 110/200 [10:40:58<8:49:44, 353.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110, Loss:  0.001223845756612718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████████                     | 111/200 [10:46:52<8:44:07, 353.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 111, Loss:  0.0007766480557620525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████████▎                    | 112/200 [10:52:46<8:38:33, 353.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 112, Loss:  0.0010160250822082162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████████▌                    | 113/200 [10:58:40<8:32:45, 353.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113, Loss:  0.004104605410248041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████████▊                    | 114/200 [11:04:34<8:27:14, 353.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114, Loss:  0.0030604382045567036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|███████████████████████████                    | 115/200 [11:10:28<8:21:26, 353.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 115, Loss:  0.005906258709728718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████▎                   | 116/200 [11:16:22<8:15:28, 353.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116, Loss:  0.0034414350520819426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████▍                   | 117/200 [11:22:16<8:09:18, 353.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 117, Loss:  0.0024423962458968163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|███████████████████████████▋                   | 118/200 [11:28:09<8:03:21, 353.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118, Loss:  0.006463471334427595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████▉                   | 119/200 [11:34:03<7:57:30, 353.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119, Loss:  0.001898060436360538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████▏                  | 120/200 [11:39:54<7:50:41, 353.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120, Loss:  0.002963004633784294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████▍                  | 121/200 [11:45:45<7:43:59, 352.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121, Loss:  0.0034769847989082336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|████████████████████████████▋                  | 122/200 [11:51:39<7:38:46, 352.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 122, Loss:  0.0031832056120038033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████▉                  | 123/200 [11:57:34<7:33:29, 353.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 123, Loss:  0.006306457333266735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|█████████████████████████████▏                 | 124/200 [12:03:28<7:27:57, 353.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, Loss:  0.0014201408484950662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|█████████████████████████████▍                 | 125/200 [12:09:22<7:22:13, 353.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125, Loss:  0.003360296133905649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|█████████████████████████████▌                 | 126/200 [12:15:16<7:16:23, 353.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126, Loss:  0.008457076735794544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|█████████████████████████████▊                 | 127/200 [12:21:10<7:10:22, 353.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 127, Loss:  0.0023582698777318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████████████████████████████                 | 128/200 [12:27:03<7:04:20, 353.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128, Loss:  0.001570763299241662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████████████████████████████▎                | 129/200 [12:32:57<6:58:27, 353.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129, Loss:  0.003262121696025133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████████████████████████████▌                | 130/200 [12:38:50<6:52:32, 353.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130, Loss:  0.0038156809750944376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████████████████████████████▊                | 131/200 [12:44:44<6:46:47, 353.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 131, Loss:  0.003319414798170328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|███████████████████████████████                | 132/200 [12:50:38<6:41:02, 353.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 132, Loss:  0.0027222782373428345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|███████████████████████████████▎               | 133/200 [12:56:33<6:35:18, 354.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 133, Loss:  0.0019655320793390274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|███████████████████████████████▍               | 134/200 [13:02:27<6:29:25, 354.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134, Loss:  0.0029732186812907457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████▋               | 135/200 [13:08:21<6:23:28, 353.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135, Loss:  0.0028029074892401695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|███████████████████████████████▉               | 136/200 [13:14:12<6:16:46, 353.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136, Loss:  0.0017283549532294273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████████▏              | 137/200 [13:20:02<6:09:56, 352.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137, Loss:  0.007180090993642807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████▍              | 138/200 [13:25:54<6:03:48, 352.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 138, Loss:  0.004707796033471823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████▋              | 139/200 [13:31:46<5:58:01, 352.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139, Loss:  0.0033805957064032555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████▉              | 140/200 [13:37:40<5:52:33, 352.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140, Loss:  0.0010161381214857101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████▏             | 141/200 [13:43:34<5:47:18, 353.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 141, Loss:  0.0018338856752961874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████████▎             | 142/200 [13:49:29<5:41:43, 353.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 142, Loss:  0.0020084853749722242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|█████████████████████████████████▌             | 143/200 [13:55:23<5:36:03, 353.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 143, Loss:  0.0009583961800672114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|█████████████████████████████████▊             | 144/200 [14:01:17<5:30:10, 353.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144, Loss:  0.0012612008722499013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████             | 145/200 [14:07:10<5:24:14, 353.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 145, Loss:  0.005425649229437113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|██████████████████████████████████▎            | 146/200 [14:13:03<5:18:08, 353.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146, Loss:  0.0027309339493513107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|██████████████████████████████████▌            | 147/200 [14:18:57<5:12:12, 353.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 147, Loss:  0.0022382917813956738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|██████████████████████████████████▊            | 148/200 [14:24:47<5:05:30, 352.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 148, Loss:  0.0024964194744825363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████████████████████████████████            | 149/200 [14:30:37<4:59:07, 351.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149, Loss:  0.0038148718886077404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████▎           | 150/200 [14:36:28<4:52:54, 351.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Loss:  0.0015234746970236301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████████████████████████████████▍           | 151/200 [14:42:19<4:46:53, 351.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 151, Loss:  0.0011690573301166296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████████████████████████████████▋           | 152/200 [14:48:12<4:41:30, 351.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 152, Loss:  0.0004375446878839284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████████████████████████████████▉           | 153/200 [14:54:07<4:36:15, 352.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 153, Loss:  0.0014533933717757463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|████████████████████████████████████▏          | 154/200 [15:00:01<4:30:43, 353.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154, Loss:  0.0016106090042740107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|████████████████████████████████████▍          | 155/200 [15:05:54<4:24:57, 353.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155, Loss:  0.002798616187646985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|████████████████████████████████████▋          | 156/200 [15:11:48<4:19:05, 353.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 156, Loss:  0.0017891761381179094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|████████████████████████████████████▉          | 157/200 [15:17:46<4:14:15, 354.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157, Loss:  0.0005312303546816111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|█████████████████████████████████████▏         | 158/200 [15:24:08<4:14:04, 362.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 158, Loss:  0.0018056329572573304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████▎         | 159/200 [15:30:29<4:11:45, 368.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 159, Loss:  0.0030863373540341854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████▌         | 160/200 [15:36:23<4:02:44, 364.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160, Loss:  0.000672612339258194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████▊         | 161/200 [15:42:16<3:54:21, 360.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 161, Loss:  0.003782593412324786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|██████████████████████████████████████         | 162/200 [15:48:08<3:46:46, 358.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 162, Loss:  0.001291363500058651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████▎        | 163/200 [15:54:00<3:39:44, 356.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 163, Loss:  0.0017635085387155414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████▌        | 164/200 [15:59:52<3:32:56, 354.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 164, Loss:  0.0039564999751746655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████▊        | 165/200 [16:05:43<3:26:21, 353.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 165, Loss:  0.001595542998984456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████████        | 166/200 [16:11:34<3:19:59, 352.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166, Loss:  0.002068055560812354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|███████████████████████████████████████▏       | 167/200 [16:17:25<3:13:51, 352.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 167, Loss:  0.0009715385385788977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|███████████████████████████████████████▍       | 168/200 [16:23:16<3:07:45, 352.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 168, Loss:  0.0009536485304124653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|███████████████████████████████████████▋       | 169/200 [16:29:19<3:03:33, 355.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 169, Loss:  0.0019606712739914656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|███████████████████████████████████████▉       | 170/200 [16:35:24<2:59:07, 358.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170, Loss:  0.0004426240921020508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████▏      | 171/200 [16:41:16<2:52:17, 356.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 171, Loss:  0.0012816942762583494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████▍      | 172/200 [16:47:08<2:45:42, 355.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172, Loss:  0.0007779753068462014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████▋      | 173/200 [16:53:02<2:39:37, 354.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 173, Loss:  0.0005166027112863958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████▉      | 174/200 [16:58:50<2:32:51, 352.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174, Loss:  0.0013417176669463515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████▏     | 175/200 [17:04:38<2:26:19, 351.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175, Loss:  0.0010984513210132718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████▎     | 176/200 [17:10:25<2:20:01, 350.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 176, Loss:  0.002021560911089182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████▌     | 177/200 [17:16:13<2:13:56, 349.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 177, Loss:  0.0017196713015437126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████▊     | 178/200 [17:22:01<2:07:58, 349.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 178, Loss:  0.001129294279962778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████     | 179/200 [17:27:51<2:02:11, 349.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 179, Loss:  0.0009721007663756609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████▎    | 180/200 [17:33:43<1:56:38, 349.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180, Loss:  0.0008337692124769092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████▌    | 181/200 [17:39:34<1:50:58, 350.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 181, Loss:  0.0016363782342523336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████▊    | 182/200 [17:45:25<1:45:11, 350.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182, Loss:  0.0023335013538599014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████    | 183/200 [17:51:16<1:39:21, 350.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 183, Loss:  0.0014726899098604918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████▏   | 184/200 [17:57:06<1:33:28, 350.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184, Loss:  0.0005389210418798029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████▍   | 185/200 [18:02:57<1:27:37, 350.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185, Loss:  0.0016868609236553311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|███████████████████████████████████████████▋   | 186/200 [18:08:47<1:21:47, 350.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 186, Loss:  0.0011949219042435288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|███████████████████████████████████████████▉   | 187/200 [18:14:38<1:15:58, 350.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 187, Loss:  0.0009343989077024162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|████████████████████████████████████████████▏  | 188/200 [18:20:29<1:10:09, 350.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 188, Loss:  0.0006434110691770911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|████████████████████████████████████████████▍  | 189/200 [18:26:20<1:04:19, 350.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 189, Loss:  0.003228316782042384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████▌  | 190/200 [18:32:12<58:31, 351.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss:  0.00024004509032238275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████▊  | 191/200 [18:38:04<52:41, 351.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 191, Loss:  0.0007884000078774989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████  | 192/200 [18:43:54<46:48, 351.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 192, Loss:  0.0012464316096156836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████▎ | 193/200 [18:49:45<40:56, 350.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 193, Loss:  0.0008300327463075519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████▌ | 194/200 [18:55:36<35:05, 350.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 194, Loss:  0.0046963463537395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|███████████████████████████████████████████████▊ | 195/200 [19:01:26<29:14, 350.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195, Loss:  0.0011693036649376154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████ | 196/200 [19:07:17<23:22, 350.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 196, Loss:  0.0005082549178041518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████████▎| 197/200 [19:13:08<17:32, 350.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 197, Loss:  0.0026659006252884865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|████████████████████████████████████████████████▌| 198/200 [19:18:58<11:41, 350.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 198, Loss:  0.0009182528010569513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████▊| 199/200 [19:24:49<05:50, 350.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199, Loss:  0.0008289127144962549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 200/200 [19:30:41<00:00, 351.21s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295dd0ff",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "960a35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "980ae0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    outputs, targets = validation(epoch)\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    precision_score_micro = metrics.precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = metrics.precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = metrics.recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = metrics.recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    auc_score_micro = metrics.roc_auc_score(targets,outputs, average='micro')\n",
    "    auc_score_macro = metrics.roc_auc_score(targets,outputs, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"Precision Score (Micro) = {precision_score_micro}\")\n",
    "    print(f\"Precision Score (Macro) = {precision_score_macro}\")\n",
    "    print(f\"Recall Score (Micro) = {recall_score_micro}\")\n",
    "    print(f\"Recall Score (Macro) = {recall_score_macro}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "    print(f\"AUC Score (Micro) = {auc_score_micro}\")\n",
    "    print(f\"AUC Score (Macro) = {auc_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c25acf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.024793388429752067\n",
      "Precision Score (Micro) = 0.5606039617675578\n",
      "Precision Score (Macro) = 0.49201423280670864\n",
      "Recall Score (Micro) = 0.4359582031670796\n",
      "Recall Score (Macro) = 0.38636549916618795\n",
      "F1 Score (Micro) = 0.490486001696764\n",
      "F1 Score (Macro) = 0.4235887817253682\n",
      "AUC Score (Micro) = 0.6951152037646923\n",
      "AUC Score (Macro) = 0.6683470912015779\n"
     ]
    }
   ],
   "source": [
    "    outputs, targets = validation(epoch)\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    precision_score_micro = metrics.precision_score(targets, outputs, average='micro')\n",
    "    precision_score_macro = metrics.precision_score(targets, outputs, average='macro')\n",
    "    recall_score_micro = metrics.recall_score(targets, outputs, average='micro')\n",
    "    recall_score_macro = metrics.recall_score(targets, outputs, average='macro')\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    auc_score_micro = metrics.roc_auc_score(targets,outputs, average='micro')\n",
    "    auc_score_macro = metrics.roc_auc_score(targets,outputs, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"Precision Score (Micro) = {precision_score_micro}\")\n",
    "    print(f\"Precision Score (Macro) = {precision_score_macro}\")\n",
    "    print(f\"Recall Score (Micro) = {recall_score_micro}\")\n",
    "    print(f\"Recall Score (Macro) = {recall_score_macro}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "    print(f\"AUC Score (Micro) = {auc_score_micro}\")\n",
    "    print(f\"AUC Score (Macro) = {auc_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bc13d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bluebert_state_dict_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e253009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"bluebert_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ac9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
